{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score, accuracy_score\n",
    ")\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment ready for baseline model training!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading preprocessed data from Segment 1...\")\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('../data/preprocessed_creditcard.csv')\n",
    "train_indices = np.load('../data/train_indices.npy')\n",
    "test_indices = np.load('../data/test_indices.npy')\n",
    "\n",
    "print(f\"âœ“ Data loaded successfully\")\n",
    "print(f\"  Dataset shape: {df.shape}\")\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Use saved indices for consistent train/test split\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = y.iloc[test_indices]\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Training set: {len(X_train):,} samples ({y_train.sum()} fraud cases)\")\n",
    "print(f\"  Test set: {len(X_test):,} samples ({y_test.sum()} fraud cases)\")\n",
    "print(f\"  Train fraud rate: {y_train.mean()*100:.3f}%\")\n",
    "print(f\"  Test fraud rate: {y_test.mean()*100:.3f}%\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING BASELINE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use these exact parameters for reproducibility and speed\n",
    "baseline_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=100,  # Keep low for speed\n",
    "    solver='liblinear',  # Fast for binary classification\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "# Time the training\n",
    "print(\"Training Logistic Regression model...\")\n",
    "start_time = time.time()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ“ Model trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "y_pred_proba = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"âœ“ Predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Business metrics\n",
    "cost_per_fp = 10  # Customer service cost\n",
    "cost_per_fn = 100  # Average fraud loss\n",
    "total_cost = fp * cost_per_fp + fn * cost_per_fn\n",
    "\n",
    "print(f\"\\nðŸ“Š Quick Performance Summary:\")\n",
    "print(f\"  Accuracy: {accuracy:.2%}\")\n",
    "print(f\"  Precision: {precision:.2%}\")\n",
    "print(f\"  Recall: {recall:.2%}\")\n",
    "print(f\"  F1-Score: {f1:.2%}\")\n",
    "print(f\"  ROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"\\nðŸ’° Business Impact:\")\n",
    "print(f\"  False Positives: {fp:,} (${fp * cost_per_fp:,} cost)\")\n",
    "print(f\"  False Negatives: {fn} (${fn * cost_per_fn:,} cost)\")\n",
    "print(f\"  Total Cost: ${total_cost:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“ˆ Creating Business Impact Dashboard...\")\n",
    "\n",
    "# Create comprehensive evaluation figure\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Subplot 1: Confusion Matrix Heatmap\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Legitimate', 'Fraud'],\n",
    "            yticklabels=['Legitimate', 'Fraud'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix\\nBaseline Model', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "# Subplot 2: Business Cost Analysis\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "categories = ['False Positives\\n(Angry Customers)', 'False Negatives\\n(Money Lost)', \n",
    "              'True Positives\\n(Fraud Caught)', 'True Negatives\\n(Happy Customers)']\n",
    "values = [fp, fn, tp, tn]\n",
    "colors = ['orange', 'red', 'green', 'lightgreen']\n",
    "bars = plt.bar(categories, values, color=colors, edgecolor='black', linewidth=1)\n",
    "plt.title('Business Impact Metrics', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + max(values)*0.01, \n",
    "             f'{val:,}', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Add cost annotation box\n",
    "cost_text = f'Total Cost: ${total_cost:,}\\n' \\\n",
    "            f'FP Cost: ${fp * cost_per_fp:,}\\n' \\\n",
    "            f'FN Cost: ${fn * cost_per_fn:,}'\n",
    "plt.text(0.98, 0.97, cost_text, transform=ax2.transAxes, \n",
    "         fontsize=11, verticalalignment='top', horizontalalignment='right',\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Subplot 3: ROC Curve\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "plt.plot(fpr, tpr, 'b-', label=f'ROC curve (AUC = {roc_auc:.3f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random classifier', alpha=0.5)\n",
    "\n",
    "# Mark current operating point (threshold = 0.5)\n",
    "default_threshold_idx = np.argmin(np.abs(thresholds - 0.5))\n",
    "plt.plot(fpr[default_threshold_idx], tpr[default_threshold_idx], 'go', \n",
    "         markersize=10, label=f'Current threshold (0.5)')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Analysis', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Threshold Analysis\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "thresholds_to_test = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "fps_list = []\n",
    "fns_list = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    y_pred_thresh = (y_pred_proba >= thresh).astype(int)\n",
    "    cm_thresh = confusion_matrix(y_test, y_pred_thresh)\n",
    "    fps_list.append(cm_thresh[0, 1])\n",
    "    fns_list.append(cm_thresh[1, 0])\n",
    "\n",
    "plt.plot(thresholds_to_test, fps_list, 'o-', label='False Positives', \n",
    "         color='orange', linewidth=2, markersize=8)\n",
    "plt.plot(thresholds_to_test, fns_list, 's-', label='False Negatives', \n",
    "         color='red', linewidth=2, markersize=8)\n",
    "plt.xlabel('Probability Threshold')\n",
    "plt.ylabel('Error Count')\n",
    "plt.title('Threshold Impact on Errors', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Mark default threshold\n",
    "plt.axvline(x=0.5, color='green', linestyle='--', alpha=0.5, linewidth=2)\n",
    "plt.text(0.51, max(max(fps_list), max(fns_list))*0.9, 'Default', \n",
    "         color='green', fontweight='bold')\n",
    "\n",
    "# Subplot 5-6: Performance Summary Box (spans 2 subplots)\n",
    "ax5 = plt.subplot(2, 3, (5, 6))\n",
    "ax5.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "{'='*55}\n",
    "         BASELINE MODEL PERFORMANCE\n",
    "{'='*55}\n",
    "\n",
    "Classification Metrics:\n",
    "  â€¢ Accuracy: {accuracy:.2%}\n",
    "  â€¢ Precision (Fraud): {precision:.2%}\n",
    "  â€¢ Recall (Fraud): {recall:.2%}\n",
    "  â€¢ F1-Score (Fraud): {f1:.2%}\n",
    "  â€¢ ROC-AUC: {roc_auc:.3f}\n",
    "\n",
    "Business Impact:\n",
    "  â€¢ False Positives: {fp:,} transactions\n",
    "    â†’ {fp:,} angry customers!\n",
    "    â†’ ${fp * cost_per_fp:,} in service costs\n",
    "  \n",
    "  â€¢ False Negatives: {fn} transactions  \n",
    "    â†’ ${fn * cost_per_fn:,} in losses!\n",
    "    \n",
    "  â€¢ Total Financial Impact: ${total_cost:,}\n",
    "  â€¢ Fraud Detection Rate: {recall:.1%}\n",
    "  â€¢ Customer Friction Rate: {fp/(tn+fp)*100:.2f}%\n",
    "\n",
    "Model Details:\n",
    "  â€¢ Algorithm: Logistic Regression (balanced)\n",
    "  â€¢ Training Time: {training_time:.2f} seconds\n",
    "  â€¢ Features Used: {X_train.shape[1]}\n",
    "\"\"\"\n",
    "\n",
    "ax5.text(0.5, 0.5, summary_text, transform=ax5.transAxes, \n",
    "         fontsize=11, ha='center', va='center',\n",
    "         bbox=dict(boxstyle='round,pad=1', facecolor='lightblue', alpha=0.8),\n",
    "         family='monospace')\n",
    "\n",
    "plt.suptitle('Baseline Fraud Detection Model - Performance Dashboard', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Dashboard created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ’¾ Saving model and metrics...\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = '../models/baseline_logistic_regression.pkl'\n",
    "joblib.dump(baseline_model, model_path)\n",
    "print(f\"âœ“ Model saved to {model_path}\")\n",
    "\n",
    "# Save metrics for comparison in Segment 3\n",
    "metrics = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'roc_auc': float(roc_auc),\n",
    "    'false_positives': int(fp),\n",
    "    'false_negatives': int(fn),\n",
    "    'true_positives': int(tp),\n",
    "    'true_negatives': int(tn),\n",
    "    'total_cost': float(total_cost),\n",
    "    'training_time': float(training_time),\n",
    "    'customer_friction_rate': float(fp/(tn+fp)*100),\n",
    "    'fraud_detection_rate': float(recall*100)\n",
    "}\n",
    "\n",
    "metrics_path = '../outputs/baseline_metrics.json'\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"âœ“ Metrics saved to {metrics_path}\")\n",
    "\n",
    "# Also save the model for use in Segment 3 comparison\n",
    "joblib.dump(baseline_model, '../models/baseline_for_comparison.pkl')\n",
    "print(\"âœ“ Comparison model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a brief summary report\n",
    "report = f\"\"\"\n",
    "# Baseline Model Report\n",
    "\n",
    "## Model Performance\n",
    "- **Algorithm**: Logistic Regression with balanced class weights\n",
    "- **Training Time**: {training_time:.2f} seconds\n",
    "- **ROC-AUC Score**: {roc_auc:.3f}\n",
    "\n",
    "## Business Metrics\n",
    "- **False Positives**: {fp:,} (customers incorrectly blocked)\n",
    "- **False Negatives**: {fn} (frauds missed)\n",
    "- **Estimated Loss**: ${total_cost:,}\n",
    "\n",
    "## Key Insights\n",
    "- The model catches {recall:.1%} of fraud cases\n",
    "- {fp/(tn+fp)*100:.2f}% of legitimate transactions are incorrectly flagged\n",
    "- Each false negative costs ~10x more than a false positive\n",
    "\n",
    "## Next Steps\n",
    "- Improve feature engineering\n",
    "- Try more sophisticated algorithms\n",
    "- Optimize the decision threshold\n",
    "\"\"\"\n",
    "\n",
    "with open('../outputs/baseline_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "print(\"âœ“ Report saved to outputs/baseline_report.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ BASELINE MODEL COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Summary:\")\n",
    "print(f\"  â€¢ Model: Logistic Regression\")\n",
    "print(f\"  â€¢ Training Time: {training_time:.2f}s\")\n",
    "print(f\"  â€¢ False Positives: {fp:,} transactions\")\n",
    "print(f\"  â€¢ False Negatives: {fn} transactions\")\n",
    "print(f\"  â€¢ Total Cost: ${total_cost:,}\")\n",
    "print(f\"  â€¢ ROC-AUC: {roc_auc:.3f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Challenge for Segment 3:\")\n",
    "print(f\"  â†’ Reduce false positives by 30% (target: <{int(fp*0.7):,})\")\n",
    "print(f\"  â†’ Maintain or improve recall (current: {recall:.1%})\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâœ… Ready for Segment 3: Model Improvement!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}